import functools
import json
from typing import Any, Literal, Optional, cast

import backoff
import openai

# from openai.openai_object import OpenAIObject

from dsp.modules.cache_utils import CacheMemory, NotebookCacheMemory, cache_turn_on
from dsp.modules.lm import LM

import sys

def backoff_hdlr(details):
    """Handler from https://pypi.org/project/backoff/"""
    print(
        "Backing off {wait:0.1f} seconds after {tries} tries "
        "calling function {target} with kwargs "
        "{kwargs}".format(**details)
    )


class GPT(LM):
    """Wrapper around OpenAI's GPT API. Supports both the OpenAI and Azure APIs.

    Args:
        model (str, optional): OpenAI or Azure supported LLM model to use. Defaults to "text-davinci-002".
        api_key (Optional[str], optional): API provider Authentication token. use Defaults to None.
        api_provider (Literal["openai", "azure"], optional): The API provider to use. Defaults to "openai".
        model_type (Literal["chat", "text"], optional): The type of model that was specified. Mainly to decide the optimal prompting strategy. Defaults to "text".
        **kwargs: Additional arguments to pass to the API provider.
    """

    def __init__(
        self,
        model: str = "text-davinci-002",
        api_key: Optional[str] = None,
        api_provider: Literal["openai", "azure"] = "openai",
        model_type: Literal["chat", "text"] = "text",
        **kwargs,
    ):
        super().__init__(model)
        self.provider = "openai"
        self.model_type = model_type

        if api_provider == "azure":
            assert (
                "engine" in kwargs or "deployment_id" in kwargs
            ), "Must specify engine or deployment_id for Azure API instead of model."
            assert "api_version" in kwargs, "Must specify api_version for Azure API"
            assert "api_base" in kwargs, "Must specify api_base for Azure API"
            openai.api_type = "azure"
            openai.api_base = kwargs["api_base"]
            if kwargs.get("api_version"):
                openai.api_version = kwargs["api_version"]

        if api_key:
            openai.api_key = api_key

        self.kwargs = {
            "temperature": 0.0,
            "max_tokens": 150,
            "top_p": 1,
            "frequency_penalty": 0,
            "presence_penalty": 0,
            "n": 1,
            **kwargs,
        }  # TODO: add kwargs above for </s>
        if api_provider == "openai":
            self.kwargs["model"] = model
        self.history: list[dict[str, Any]] = []

    def basic_request(self, prompt: str, **kwargs):
        raw_kwargs = kwargs

        kwargs = {**self.kwargs, **kwargs}
        if self.model_type == "chat":
            # caching mechanism requires hashable kwargs
            kwargs["messages"] = [{"role": "user", "content": prompt}]
            kwargs = {
                "stringify_request": json.dumps(kwargs)
            }
            response = cached_gpt_turbo_request(**kwargs)

        else:
            kwargs["prompt"] = prompt
            response = cached_gpt_request(**kwargs)

        history = {
            "prompt": prompt,
            "response": response,
            "kwargs": kwargs,
            "raw_kwargs": raw_kwargs,
        }
        self.history.append(history)

        return response

    @backoff.on_exception(
        backoff.expo,
        (openai.APIError, openai.APIConnectionError),
        max_time=1000,
        on_backoff=backoff_hdlr,
    )
    def request(self, prompt: str, **kwargs):
        """Handles retreival of GPT-3 completions whilst handling rate limiting and caching."""
        if "model_type" in kwargs:
            del kwargs["model_type"]
        
        return self.basic_request(prompt, **kwargs)

    def _get_choice_text(self, choice: dict[str, Any]) -> str:
        if self.model_type == "chat":
            return choice.message.content
        return choice.text

    def __call__(
        self,
        prompt: str,
        only_completed: bool = True,
        return_sorted: bool = False,
        **kwargs,
    ) -> list[dict[str, Any]]:
        """Retrieves completions from GPT-3.

        Args:
            prompt (str): prompt to send to GPT-3
            only_completed (bool, optional): return only completed responses and ignores completion due to length. Defaults to True.
            return_sorted (bool, optional): sort the completion choices using the returned probabilities. Defaults to False.

        Returns:
            list[dict[str, Any]]: list of completion choices
        """

        assert only_completed, "for now"
        assert return_sorted is False, "for now"

        if kwargs.get("n", 1) > 1:
            if self.model_type == "chat":
                kwargs = {**kwargs}
            else:
                kwargs = {**kwargs, "logprobs": 5}

        response = self.request(prompt, **kwargs)
        choices = response.choices

        completed_choices = [c for c in choices if c.finish_reason != "length"]

        if only_completed and len(completed_choices):
            choices = completed_choices

        completions = [self._get_choice_text(c) for c in choices]

        if return_sorted and kwargs.get("n", 1) > 1:
            scored_completions = []

            for c in choices:
                tokens, logprobs = (
                    c["logprobs"]["tokens"],
                    c["logprobs"]["token_logprobs"],
                )

                if "<|endoftext|>" in tokens:
                    index = tokens.index("<|endoftext|>") + 1
                    tokens, logprobs = tokens[:index], logprobs[:index]

                avglog = sum(logprobs) / len(logprobs)
                scored_completions.append((avglog, self._get_choice_text(c)))

            scored_completions = sorted(scored_completions, reverse=True)
            completions = [c for _, c in scored_completions]

        return completions

@functools.lru_cache(maxsize=None if cache_turn_on else 0)
@CacheMemory.cache
def cached_gpt_request_v2(**kwargs):
    return openai.completions.create(**kwargs)

'''
@functools.lru_cache(maxsize=None if cache_turn_on else 0)
@NotebookCacheMemory.cache
def cached_gpt_request_v2_wrapped(**kwargs):
    response = cached_gpt_request_v2(**kwargs)
    return response
'''
def cached_gpt_request_v2_logged(**kwargs):
    print("calling chatgpt ...", file=sys.stderr)
    print("~"*35 + " PROMPT " + "~"*35)
    print(kwargs["prompt"])
    response = cached_gpt_request_v2(**kwargs)
    print("-"*35 + " RESPONSE " + "-"*35)
    for i, choice in enumerate(response.choices):
        print("-"*35 + (" CHOICE %d "%i) + "-"*35)
        print(choice.text)
    return response


cached_gpt_request = cached_gpt_request_v2_logged

@functools.lru_cache(maxsize=None if cache_turn_on else 0)
@CacheMemory.cache
def _cached_gpt_turbo_request_v2(**kwargs):
    if "stringify_request" in kwargs:
        kwargs = json.loads(kwargs["stringify_request"])
    print(kwargs, file=sys.stderr)
    return openai.chat.completions.create(**kwargs)

'''
@functools.lru_cache(maxsize=None if cache_turn_on else 0)
@NotebookCacheMemory.cache
def _cached_gpt_turbo_request_v2_wrapped(**kwargs) -> OpenAIObject:
    response = _cached_gpt_turbo_request_v2(**kwargs)
    return response
'''

def _cached_gpt_turbo_request_v2_logged(**kwargs):
    print("calling chatgpt ...", file=sys.stderr)
    print("~"*35 + " PROMPT " + "~"*35)
    for message in json.loads(kwargs["stringify_request"])["messages"]:
        print("."*35 + message['role'] + "."*35)
        print(message['content'])
    response = _cached_gpt_turbo_request_v2(**kwargs)
    print("-"*35 + " RESPONSE " + "-"*35)
    for i, choice in enumerate(response.choices):
        print("-"*35 + (" CHOICE %d "%i) + "-"*35)
        print("."*35 + choice.message.role + "."*35)
        print(choice.message.content)
    return response

cached_gpt_turbo_request = _cached_gpt_turbo_request_v2_logged
